{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for OpenAI Gym Pong\n",
    "[Based on the blog by Karpathy on Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# log the information to a text file\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, filename=\"Default.log\"):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.log.flush()\n",
    "\n",
    "sys.stdout = Logger(\"tf_log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define same basic CNN helfer functions\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "image_size = 80 \n",
    "D = image_size * image_size # input dimensionality: 80x80 grid\n",
    "\n",
    "# model definition in tensorflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "observations = tf.placeholder(tf.float32, [None, D] , name=\"input_x\")\n",
    "\n",
    "\n",
    "x_image = tf.reshape(observations, [-1,image_size,image_size,1])\n",
    "\n",
    "# define the first layer: convolution + ReLU\n",
    "W_conv1 = tf.get_variable(\"W1\", shape=[12,12,1,32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# define the second layer: convolution + ReLU\n",
    "W_conv2 = tf.get_variable(\"W2\", shape=[8,8,32,48], initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# define the third layer: densely connected layer\n",
    "W_fc1 = tf.get_variable(\"W3\", shape=[20 * 20 * 48, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 20*20*48])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1))\n",
    "\n",
    "# softmax laye\n",
    "W_fc2 = tf.get_variable(\"W4\", shape=[256, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "y_conv = tf.matmul(h_fc1, W_fc2)\n",
    "\n",
    "# now we get the probability of moving up\n",
    "probability = tf.nn.sigmoid(y_conv)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loss = tf.reduce_mean((tf.square(input_y - probability) * advantages) )\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=0.001) # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_gradW1\") # Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_gradW2\")\n",
    "W3Grad = tf.placeholder(tf.float32,name=\"batch_gradW3\")\n",
    "W4Grad = tf.placeholder(tf.float32,name=\"batch_gradW4\")\n",
    "batchGrad = [W1Grad,W2Grad,W3Grad, W4Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,ys,drs = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess =  tf.Session()\n",
    "sess.run(init)\n",
    "gradBuffer = sess.run(tvars)\n",
    "\n",
    "for ix in range(len(gradBuffer)): gradBuffer[ix] = 0.0 * gradBuffer[ix]\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if resume:\n",
    "    saver.restore(sess, \"TF_RL_save.p\")\n",
    "\n",
    "won = 0\n",
    "lost = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.monitor.start('/root/tmp/pong-experiment-1',force=1)\n",
    "env.reset()\n",
    "\n",
    "while episode_number <= 4000:\n",
    "\n",
    "  if render: env.render()\n",
    "\n",
    "  # Reset the gradient placeholder. We will collect gradients in \n",
    "  # gradBuffer until we are ready to update our policy network. \n",
    "  #gradBuffer = sess.run(tvars)\n",
    "\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  x = x.reshape([1,80*80])\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "\n",
    "  action = 2 if np.random.uniform() < tfprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  #hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  ys.append(y)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if len(drs) > 3000:\n",
    "    drs.pop(0)\n",
    "    ys.pop(0)\n",
    "    xs.pop(0)\n",
    "       \n",
    "  if reward != 0:\n",
    "    if reward > 0: won+=1 \n",
    "    else: lost+=1\n",
    "\n",
    "  if done: # an episode finished\n",
    "    \n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    epy = np.vstack(ys)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,ys,drs = [],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    \n",
    "    # Get the gradient for this episode, and save it in the gradBuffer\n",
    "    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "    for ix,grad in enumerate(tGrad): gradBuffer[ix] += grad  \n",
    "\n",
    "    # perform parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "           \n",
    "        sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1], W3Grad: gradBuffer[2], W4Grad: gradBuffer[3]})#,W3Grad: gradBuffer[2],W4Grad:gradBuffer[3],W5Grad:gradBuffer[4]})\n",
    "\n",
    "        # reset batch gradient buffer\n",
    "        for ix in range(len(gradBuffer)): gradBuffer[ix] = 0.0 * gradBuffer[ix]\n",
    "\n",
    "    # book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    if episode_number % 10 == 0: print 'resetting env. episode %d reward total was %f. running mean: %f, won: %.1f %%' % (episode_number, reward_sum, running_reward, 100.0*float(won)/float(won+lost))\n",
    "    if episode_number % 100 == 0: save_path = saver.save(sess, \"TF_RL_save.p\")\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "env.monitor.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "plt.plot(step_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
